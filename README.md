# ğŸš€ GitHub Crawler

A fully automated GitHub GraphQL crawler that collects repository data, stores it in PostgreSQL, and exports the results as an artifact via GitHub Actions CI/CD.

---

## ğŸ“˜ Overview

This project demonstrates:
- Efficient use of the **GitHub GraphQL API** for large-scale repository crawling  
- Automated **PostgreSQL** setup and schema creation  
- Continuous-integration data pipeline using **GitHub Actions**  
- Export of database contents as a **CSV artifact**

It meets all assignment requirements:  
âœ… PostgreSQL service container  
âœ… Dependency installation  
âœ… Setup-DB step  
âœ… Crawl-Stars step (GraphQL API)  
âœ… Artifact upload  
âœ… Runs with default GitHub token (no secrets)

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|--------|-------------|
| Language | Node.js (ES Modules) |
| API | GitHub GraphQL v4 |
| Database | PostgreSQL 15 |
| CI/CD | GitHub Actions |
| Package Manager | npm |

---

## âš™ï¸ Local Setup Guide

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/moiz-habib/github-crawler.git
cd github-crawler
```
### 2ï¸âƒ£ Install Dependencies
```bash
npm install
```
### 3ï¸âƒ£ Set Up PostgreSQL Locally
```bash
# Example local credentials
DB_USER=postgres
DB_PASS=12345
DB_NAME=github
```
### 4ï¸âƒ£ Initialize the Database
```bash
npm run setup-db
```
### 5ï¸âƒ£ Provide a GitHub Token
Use any public-repo personal access token (only needed locally):
Windows (PowerShell)
```bash
$env:GITHUB_TOKEN = "your_token_here"
```
### 6ï¸âƒ£ Run the Crawler Locally
```bash
npm run crawl
```
